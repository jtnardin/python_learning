{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading nlp/data/wiki-news-300d-100k.vec...\n",
      "Loaded 100000 words.\n",
      "Removed stop words, 98650 remain.\n",
      "Removed duplicates, 97192 remain.\n",
      "Words related to section:\n",
      "subsection, sections, paragraph, subheading, subsections, heading, portion, paragraphs, Background, Sections\n",
      "Words related to question:\n",
      "answer, questions, ask, whether, answers, answering, issue, answered, conundrum, asking\n",
      "Words related to development:\n",
      "growth, developments, implementation, developement, construction, developing, advancement, research, developmental, design\n",
      "Words related to staff:\n",
      "staffs, personnel, staffers, faculty, members, employees, assistants, staffer, officers, consultants\n",
      "\n",
      "man-him is like woman-her\n",
      "quick-quickest is like far-furthest\n",
      "sushi-rice is like pizza-wheat\n",
      "Paris-France is like Rome-Italy\n",
      "dog-mammal is like eagle-bird\n",
      "German-BMW is like American-Lexus\n",
      "German-Opel is like American-Chrysler\n"
     ]
    }
   ],
   "source": [
    "run nlp/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from typing import List\n",
    "import vectors as v\n",
    "from vectors import Vector\n",
    "\n",
    "class Word:\n",
    "    def __init__(self,text:str,vector:Vector) -> None:\n",
    "        self.text = text\n",
    "        self.vector = vector\n",
    "        \n",
    "def vector_len(v: Vector) -> float:\n",
    "    return math.sqrt(sum([x*x for x in v]))\n",
    "\n",
    "def dot_product(v1: Vector, v2: Vector) -> float:\n",
    "    assert len(v1) == len(v2)\n",
    "    return sum([x*y for (x,y) in zip(v1, v2)])\n",
    "\n",
    "def cosine_similarity(v1: Vector, v2: Vector) -> float:\n",
    "    #Returns the cosine of the angle between the two vectors.\n",
    "    #Results range from -1 (very different) to 1 (very similar).\n",
    "    return dot_product(v1, v2) / (vector_len(v1) * vector_len(v2))\n",
    "\n",
    "def sorted_by_similarity(words: List[Word], base_vector: Vector) -> List[Tuple[float, Word]]:\n",
    "    #Returns words sorted by cosine distance to a given vector, most similar first\"\"\"\n",
    "    words_with_distance = [(cosine_similarity(base_vector, w.vector), w) for w in words]\n",
    "    # We want cosine similarity to be as large as possible (close to 1)\n",
    "    return sorted(words_with_distance, key=lambda t: t[0], reverse=True)\n",
    "\n",
    "def print_related(words: List[Word], text: str) -> None:\n",
    "    base_word = find_word(words,text)\n",
    "    sorted_words = [\n",
    "        word.text for (dist, word) in\n",
    "            sorted_by_similarity(words, base_word.vector)\n",
    "            if word.text.lower() != base_word.text.lower()\n",
    "        ]\n",
    "    print(', '.join(sorted_words[:7]))\n",
    "    \n",
    "\n",
    "def print_related_str(words: List[Word], text: str) -> str:\n",
    "    base_word = find_word(words,text)\n",
    "    sorted_words = [\n",
    "        word.text for (dist, word) in\n",
    "            sorted_by_similarity(words, base_word.vector)\n",
    "            if word.text.lower() != base_word.text.lower()\n",
    "        ]\n",
    "    return ', '.join(sorted_words[:7])\n",
    "    \n",
    "def find_word(words: List[Word], text: str) -> Word:\n",
    "    return next(w for w in words if text == w.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading nlp/data/wiki-news-300d-100k.vec...\n",
      "Loaded 100000 words.\n",
      "Removed stop words, 98650 remain.\n",
      "Removed duplicates, 97192 remain.\n"
     ]
    }
   ],
   "source": [
    "#Vector = List[float]\n",
    "               \n",
    "words = load_words('nlp/data/wiki-news-300d-100k.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the word of interest is \"bee\"\n",
      "the related words include: bees, honeybee, bumblebee, hive, wasp, honey, ant\n"
     ]
    }
   ],
   "source": [
    "word_of_interest = \"bee\";\n",
    "print(\"the word of interest is \\\"\" + word_of_interest + \"\\\"\" )\n",
    "print(\"the related words include: \" + print_related_str(words,word_of_interest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest_analogies(\n",
    "    left2: str, left1: str, right2: str, words: List[Word]\n",
    ") -> List[Tuple[float, Word]]:\n",
    "    word_left1 = find_word(words,left1)\n",
    "    word_left2 = find_word(words,left2)\n",
    "    word_right2 = find_word(words,right2)\n",
    "    vector =  v.add(\n",
    "        v.sub(word_left1.vector, word_left2.vector),\n",
    "        word_right2.vector)\n",
    "    closest = sorted_by_similarity(words, vector)[:10]\n",
    "    def is_redundant(word: str) -> bool:\n",
    "        #Sometimes the two left vectors are so close the answer is e.g.\n",
    "        #\"shirt-clothing is like phone-phones\". Skip 'phones' and get the next\n",
    "        #suggestion, which might be more interesting.\n",
    "        return (\n",
    "            left1.lower() in word.lower() or\n",
    "            left2.lower() in word.lower() or\n",
    "            right2.lower() in word.lower())\n",
    "    return [(dist, w) for (dist, w) in closest if not is_redundant(w.text)]\n",
    "\n",
    "def print_analogy(left2: str, left1: str, right2: str, words: List[Word]) -> None:\n",
    "    analogies = closest_analogies(left2, left1, right2, words)\n",
    "    if (len(analogies) == 0):\n",
    "        print(f\"{left2}-{left1} is like {right2}-?\")\n",
    "    else:\n",
    "        (dist, w) = analogies[0]\n",
    "        print(f\"{left2} is to {left1} as {right2} is to {w.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apple is to apples as puppy is to puppies\n"
     ]
    }
   ],
   "source": [
    "#good\n",
    "#print_analogy(\"paw\",\"kitten\",\"hoof\",words)\n",
    "#print_analogy(\"Paris\",\"France\",\"Delhi\",words)\n",
    "#print_analogy(\"apple\",\"fruit\",\"celery\",words)\n",
    "#print_analogy(\"apple\",\"apples\",\"goose\",words)\n",
    "\n",
    "print_analogy(\"apple\",\"apples\",\"puppy\",words)\n",
    "\n",
    "#bad\n",
    "#print_analogy(\"yogurt\",\"dairy\",\"cereal\",words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
